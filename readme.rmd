---
title: "PyArrow Read S3 Parquet Benchmarks"
output: github_document
date: "2/14/2021"
---

I was implementing a system that reads single parquet files from S3 and was
surprised how slow it could be. These benchmarks show how the performance of 
reading parquet files from S3 differs from a local filesystem.

I tested both PyArrow and AWS Data Wrangler's reader. AWS Data Wrangler wraps
PyArrow's parquet reader, but implements [it's own fancy S3 filesystem abstraction](https://github.com/awslabs/aws-data-wrangler/blob/d5bf86f8a6a7c839224fd4ae6f4aa0305846cc70/awswrangler/s3/_fs.py#L573).
So we are comparing their S3 interaction, not their deserialization of the parquet format. 

One caveat on the Data Wrangler implementation: they only expose functions that 
return Pandas DataFrames, so the run time method's benchmarks includes time spent
translating from Arrow to Pandas and back to Arrow.

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE)

library(arrow)
library(ggplot2)
library(dplyr)
library(gt)
```

```{r load data}
results <- read_parquet('results.parquet') %>%
  mutate(num_columns = as.factor(ifelse(is.na(num_columns), 'All', as.character(num_columns))),
         throughput_mbps = out_size / 1024 / 1024 / runtime)

ggplot(results, aes(x = method, y = throughput_mbps, fill = num_columns)) +
  geom_col(position='dodge') +
  scale_y_log10() +
  labs(
    title="Throughput (MBps) of Reading Parquet from S3",
    caption="Throughput defined as result size in MB divided by load time",
    y="Throughput (MBps)",
    fill="Number of Columns"
  )
  #facet_grid(rows = vars(num_columns), cols = vars(num_files))

ggplot(results, aes(x = method, y = runtime, fill = num_columns)) +
  geom_col(position='dodge') +
  labs(
    title="Time to Reading Parquet from S3",
    caption="Throughput defined as result size in MB divided by load time",
    y="Run time (s)",
    fill="Number of Columns"
  )
```

```{r}
results %>%
  mutate(out_size_mb = out_size / 1024 / 1024) %>%
  select(
    method,
    num_columns,
    out_size_mb,
    runtime,
    throughput_mbps
  ) %>%
  group_by(method) %>%
  gt() %>%
  tab_header(
    title = "S3 Parquet Read Benchmark Results"
  ) %>%
  fmt_number(
    vars(runtime, out_size_mb, throughput_mbps)
  ) %>%
  cols_label(
    num_columns = "Num Columns Read",
    out_size_mb = "Result Size (MB)",
    runtime = "Runtime (s)",
    throughput_mbps = "Throughput (MBps)"
  ) %>%
  as_raw_html()
```

## Discussion

A few initial conclusions:

First, reading from a local filesystem is *far* more efficient than from S3. If 
you plan to keep the files around and read multiple times, it's likely worth just 
downloading the whole dataset.

Second, AWS Data Wrangler appears to be exceptionally good at reading whole
parquet files, but is actually worse than PyArrow's s3fs at reading column subsets.
Notably, it's faster to read the whole file with Data Wrangler than read only 4
columns with either method.

PyArrow's parquet reader may see meaningful improvements when ARROW-11601 is
complete: https://github.com/apache/arrow/pull/9482.

The performance issues with Parquet reads in S3 have also been discussed here:

 * https://issues.apache.org/jira/browse/PARQUET-1698

## Setup

Install requirements in virtual environment:

```bash
python -m venv .venv
source .venv/bin/activate
python -m pip install -r requirements.txt
```

Then run the setup script

```bash
python cli init
```

Now you can run the benchmark:

```bash
python cli run
```

When you are finished, cleanup with:

```bash
python cli cleanup
```